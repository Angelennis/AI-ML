# ## Importing the necessary libraries

## Installing the libraries with the specified version.
# Uncomment and run the following line if Google Colab is being used
# !pip install numpy==1.25.2 pandas==1.5.3 -q --user

# If running locally uncomment and run any needed modules 
## YOU MUST HAVE GPUs
# !pip install requests
# !pip install numpy
# !pip install pandas
# !pip install huggingface-hub 
# !pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113

# # Installing llama-cpp-python with the required environment variables
# !CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-python --force-reinstall --no-cache-dir -q

# to read and manipulate the data
import requests
import pandas as pd
import numpy as np
pd.set_option('max_colwidth', None)  # setting column to the maximum column width as per the data

# API endpoint URL
url = 'https://services.nvd.nist.gov/rest/json/source/2.0'

# Make a GET request to fetch the data
response = requests.get(url)
response.raise_for_status()  # This will raise an exception if the request failed

# Convert the JSON data to a pandas DataFrame
data = response.json()  # Parse JSON data from the response

# Print out the keys of the JSON response to understand its structure
print("JSON Keys:", data.keys())

# Print the first few characters of the JSON data to inspect its structure
print("JSON Data Sample:", str(data)[:500])

# Extract the 'sources' key from the JSON response if it exists
if 'sources' in data:
    sources_data = data['sources']
else:
    sources_data = []

# Treat each dictionary within 'sources_data' as a separate row
cvedata = pd.json_normalize(sources_data)

# ### Installing and Importing Necessary Libraries
# Installation for GPU llama-cpp-python
from huggingface_hub import hf_hub_download
from llama_cpp import Llama

# ### LLM - Llama
# #### Loading the model
## Model configuration
model_name_or_path = "TheBloke/Llama-2-13B-chat-GGUF"
model_basename = "llama-2-13b-chat.Q5_K_M.gguf"
model_path = hf_hub_download(
    repo_id=model_name_or_path,
    filename=model_basename
)

lcpp_llm = Llama(
    model_path=model_path,
    n_threads=2,  # CPU cores
    n_batch=512,  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.
    n_gpu_layers=43,  # Change this value based on your model and your GPU VRAM pool.
    n_ctx=4096,  # Context window
)

# function to generate, process, and return the response from the LLM
def generate_llm_response(user_prompt):
    # System message
    system_message = """
    [INST]<<SYS>> Respond to the user question based on the user prompt<</SYS>>[/INST]
    """
    # Combine user_prompt and system_message to create the prompt
    prompt = f"{user_prompt}\n{system_message}"

    # Generate a response from the LLaMA model
    response = lcpp_llm(
        prompt=prompt,
        max_tokens=1024,
        temperature=0.01,
        stop=['INST'],
        echo=False
    )
    # Extract and return the response text
    response_text = response["choices"][0]["text"]
    return response_text

# #### Examples
# **Question-1**
user_prompt = "How many CVEs affect Adobe Systems Incorporated?"
response = generate_llm_response(user_prompt)
print("LLM - Llama model - Question-1")
print(response)

# **Question-2**
user_prompt = "A brief overview of network vulnerabilities"
response = generate_llm_response(user_prompt)
print("LLM - Llama model - Question-2")
print(response)

# **Question-3**
user_prompt = "List the most vulnerable products according to the National Vulnerability Database."
response = generate_llm_response(user_prompt)
print("LLM - Llama model - Question-3")
print(response)

# ### LLM - Mistral
# #### Loading the model
model_name_or_path = "TheBloke/Mistral-7B-Instruct-v0.2-GGUF"
model_basename = "mistral-7b-instruct-v0.2.Q6_K.gguf"

model_path = hf_hub_download(
    repo_id=model_name_or_path,
    filename=model_basename
)

llm = Llama(
    model_path=model_path,
    n_ctx=1024,
)

# **We are going to use an instruction-tuned Mistral model. Hence, the format of the input to the model varies from that of Llama.**
# Defining the response function for Task 1.
def generate_llm_response(prompt):
    model_output = llm(
      f"""
      Q: {prompt}
      A:
      """,
      max_tokens=1024,
      temperature=0.01,
      echo=False,
    )
    temp_output = model_output["choices"][0]["text"]
    return temp_output

# #### Examples
# **Question-1**
user_prompt = "How many CVEs affect Adobe Systems Incorporated?"
response = generate_llm_response(user_prompt)
print("Mistral model - Question-1")
print(response)

# **Question-2**
user_prompt = "A brief overview of network vulnerabilities"
response = generate_llm_response(user_prompt)
print("Mistral model - Question-2")
print(response)

# **Question-3**
user_prompt = "List the most vulnerable products according to the National Vulnerability Database."
response = generate_llm_response(user_prompt)
print("Mistral model - Question-3")
print(response)
